# @package _group_

batch_size: 32
# batch_size: 16
# batch_size: 8
dev_batch_size: 64
adam_eps: 1e-8
adam_betas: (0.9, 0.999)
max_grad_norm: 2.0
log_batch_step: 1
train_rolling_loss_step: 100
weight_decay: 0.0
learning_rate: 1e-5
# learning_rate: 5e-6

# Linear warmup over warmup_steps.
# warmup_steps: 2474 # bs 8
# warmup_steps: 1237 # bs 16
warmup_steps: 618 # bs 32
# warmup_steps: 309 # bs 64

# Number of updates steps to accumulate before performing a backward/update pass.
gradient_accumulation_steps: 1

shuffle_positives_override: True
# Total number of training epochs to perform.
num_train_epochs: 100
eval_per_epoch: 1
hard_negatives: 1 # still used during epoch val nll_loss calc
precomputed_hard_negatives: 1 # precomputed before epoch
# precomputed_hard_negatives: 2 # precomputed before epoch
# precomputed_hard_negatives: 8 # precomputed before epoch
# precomputed_hard_negatives: 16 # precomputed before epoch
# precomputed_hard_negatives: 32 # precomputed before epoch
other_negatives: 0
val_av_rank_hard_neg: 30
val_av_rank_other_neg: 30
val_av_rank_bsz: 128
val_av_rank_max_qs: 10000
